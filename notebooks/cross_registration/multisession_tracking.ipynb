{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.getipython import get_ipython\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "sys.path.append(\"..\")\n",
    "from placecode import utils as ut\n",
    "from placecode.from_caiman import *\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "        get_ipython().run_line_magic('autoreload', '2')\n",
    "except NameError:\n",
    "    pass\n",
    "from datetime import datetime\n",
    "import scipy\n",
    "from scipy.ndimage import gaussian_filter1d  # smooth signal strength maps\n",
    "\n",
    "sns.set(font_scale=3)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figs = True\n",
    "if save_figs:\n",
    "    file_extension = \".pdf\"\n",
    "    output_folder = ut.open_dir(\"Choose folder to save figures\")\n",
    "    print(f\"Saving figures as {file_extension} is turned on. Saving figures to {output_folder}\")\n",
    "    now = datetime.now()\n",
    "    datetime_str = f\"{now.year:04}{datetime.now().month:02}{datetime.now().day:02}-{datetime.now().hour:02}{datetime.now().minute:02}{datetime.now().second:02}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do a heatmap plotting all persistent cells (that made through analysis): each column is condition, each row contains same cell spatial component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma=2  # sigma for gaussian 1d smoothing of signal strength map (firing rate map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open (hdf5) files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = []\n",
    "while True:\n",
    "    fpath = ut.open_file(\"Open hdf5 file, or press Cancel to finish\")\n",
    "    if fpath == \".\":  # user pressed cancel\n",
    "        break\n",
    "    else:\n",
    "        files_list.append(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_list = []\n",
    "A_list = []\n",
    "dims_list = []  # Cn entry in workspace # TODO: A_sparse always have lower resolution, probably from cropping... should I define that as dims?\n",
    "templates = []\n",
    "p_vals = []\n",
    "conditions = []\n",
    "tv_angles = []\n",
    "tv_lengths = []\n",
    "ssm_zs = []\n",
    "ssm_event_masks = []\n",
    "mouse_ids = []\n",
    "for fpath in files_list:\n",
    "    with h5py.File(fpath, \"r\") as hf:\n",
    "        mouse_id = hf.attrs[\"mouse_id\"]\n",
    "        resolution = hf.attrs[\"resolution\"][()]\n",
    "        n_components = hf.attrs[\"n_units\"]\n",
    "        condition = hf.attrs[\"condition\"]\n",
    "        ps = hf[\"p_values_tuned\"][()]\n",
    "        A_data = hf[\"A_data\"][()]\n",
    "        A_indices = hf[\"A_indices\"][()]\n",
    "        A_indptr = hf[\"A_indptr\"][()]\n",
    "        A_shape = hf[\"A_shape\"][()]\n",
    "        tv_a = hf[\"tuned_vector_angles\"][()]\n",
    "        tv_l = hf[\"tuned_vector_lengths\"][()]\n",
    "        ssm_z = hf[\"ssm_z\"][()]\n",
    "        ssm_event_mask = hf[\"ssm_events_mask\"][()]\n",
    "        #spatial = ut.read_spatial(A_data, A_indices, A_indptr, A_shape, n_components, resolution, unflatten=False)\n",
    "        spatial = scipy.sparse.csc_matrix((A_data, A_indices, A_indptr), shape=A_shape)\n",
    "        dims_list.append(resolution)\n",
    "        A_list.append(spatial)  # need to swap: (n_units, n_pixels) -> (n_pixels, n_units)\n",
    "        p_vals.append(ps)\n",
    "        conditions.append(condition)\n",
    "        tv_angles.append(tv_a)\n",
    "        tv_lengths.append(tv_l)\n",
    "        ssm_zs.append(ssm_z)\n",
    "        mouse_ids.append(mouse_id)\n",
    "        ssm_event_masks.append(ssm_event_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m_id in mouse_ids[1:]:  # make sure all data belongs to same mouse\n",
    "    assert m_id == mouse_ids[0]\n",
    "mouse_id = mouse_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tuned vector data into numpy array. To deal with varying number of units per recording (condition), pad each column to the longest with np.nan\n",
    "max_len = max(len(lst) for lst in tv_angles) \n",
    "def convert_to_np(list_of_arrs):\n",
    "    \"\"\"\n",
    "    given a list of 1D arrays, convert to a 2D array, add padding with np.nans to achieve equal column sizes \n",
    "    \"\"\"\n",
    "    return np.array([np.concatenate([lst, [np.nan]*(max_len - len(lst))]) for lst in list_of_arrs]).T\n",
    "\n",
    "tv_angles_padded = convert_to_np(tv_angles)\n",
    "tv_lengths_padded = convert_to_np(tv_lengths)\n",
    "p_vals_padded = convert_to_np(p_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(templates) > 0:\n",
    "    templates_cropped = []\n",
    "    for template in templates:\n",
    "        FOV_shape = template.shape\n",
    "        cropped_shape = dims_list[0]\n",
    "        \n",
    "        x_crop_onesided = (FOV_shape[0] - cropped_shape[0])//2\n",
    "        assert 2*x_crop_onesided == FOV_shape[0] - cropped_shape[0]\n",
    "\n",
    "        y_crop_onesided = (FOV_shape[1] - cropped_shape[1])//2\n",
    "        assert 2*y_crop_onesided == FOV_shape[1] - cropped_shape[1]\n",
    "        template_cropped = template[y_crop_onesided:-y_crop_onesided,x_crop_onesided:-x_crop_onesided]  # TODO: x and y swapped?\n",
    "        templates_cropped.append(template_cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `register_multisession()`\n",
    "\n",
    "The function `register_multisession()` requires 3 arguments:\n",
    "- `A`: A list of ndarrays or scipy.sparse.csc matrices with (# pixels X # component ROIs) for each session\n",
    "- `dims`: Dimensions of the FOV, needed to restore spatial components to a 2D image\n",
    "- `templates`: List of ndarray matrices of size `dims`, template image of each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_union, assignments, matchings = register_multisession(A=A_list, dims=dims_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns 3 variables for further analysis:\n",
    "- `spatial_union`: csc_matrix (# pixels X # total distinct components), the union of all ROIs across all sessions aligned to the FOV of the last session.\n",
    "- `assignments`: ndarray (# total distinct components X # sessions). `assignments[i,j]=k` means that component `k` from session `j` has been identified as component `i` from the union of all components, otherwise it takes a `NaN` value. Note that for each `i` there is at least one session index `j` where `assignments[i,j]!=NaN`.\n",
    "- `matchings`: list of (# sessions) lists. Saves `spatial_union` indices of individual components in each session. `matchings[j][k] = i` means that component `k` from session `j` is represented by component `i` in the union of all components `spatial_union`. In other words `assignments[matchings[j][k], j] = j`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create various subgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_conditions = len(conditions)\n",
    "assignments_filtered = assignments[~np.isnan(assignments).all(axis=1)]  # filter out rows full of np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take only omnipresent cells\n",
    "(omnipresent cell = cell that could be identified in all recordings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments_omnipresent = assignments_filtered[~np.isnan(assignments_filtered).any(axis=1)].astype(np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match (pair) values for same cell from different conditions (recordings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each omnipresent unit, get the vector length for each included condition\n",
    "tv_lengths_paired = np.zeros(assignments_omnipresent.shape)\n",
    "tv_angles_paired = np.zeros(assignments_omnipresent.shape)\n",
    "p_vals_paired = np.zeros(assignments_omnipresent.shape)\n",
    "for i_cond in range(len(conditions)):\n",
    "    tv_lengths_paired[:, i_cond] = tv_lengths_padded[ assignments_omnipresent.T[i_cond],i_cond]\n",
    "    tv_angles_paired[:, i_cond] = tv_angles_padded[ assignments_omnipresent.T[i_cond],i_cond]\n",
    "    p_vals_paired[:, i_cond] = p_vals_padded[assignments_omnipresent.T[i_cond], i_cond]\n",
    "\n",
    "# check that np.nans (coming from analysis where cells did not fulfill criteria to be included) match for all variables\n",
    "assert (~np.isnan(p_vals_paired).any(axis=1) == ~np.isnan(tv_angles_paired).any(axis=1) ).all()\n",
    "assert (~np.isnan(p_vals_paired).any(axis=1) == ~np.isnan(tv_lengths_paired).any(axis=1) ).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get persistent cels\n",
    "(persistent cell = omnipresent cell that fulfilled requirement for getting included in place coding analysis for each condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_persistent = ~np.isnan(p_vals_paired).any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_lengths_persistent = tv_lengths_paired[i_persistent]\n",
    "tv_angles_persistent = tv_angles_paired[i_persistent]\n",
    "p_vals_persistent = p_vals_paired[i_persistent]\n",
    "assignments_persistent = assignments_omnipresent[i_persistent]\n",
    "\n",
    "assert tv_lengths_persistent.shape == tv_angles_persistent.shape\n",
    "assert tv_angles_persistent.shape == p_vals_persistent.shape\n",
    "assert p_vals_persistent.shape == assignments_persistent.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `assignments_persistent` contains one row per persistent cell where it fulfilled analysis criteria (minimum number of events...) for all included conditions. For each row, each column contains the original cell index in the recording of the corresponding `conditions_to_use` condition (i.e. `assignments_persistent[0][0]==8` means the first persistent cell is cell 8 (with indexing starting at 0) in the baseline recording. The same cell might be cell 253 in the second condition (`assignments_persistent[0][1]==253`) )\n",
    "* `tv_lengths_persistent`, `tv_angles_persistent`, `p_vals_persistent` contain the tuning vector lengths, angles, and the p value, each row one neuron tracked over the conditions (that fulfilled analysis criteria). The rows and columns match those of `assignments_persistent` (i.e. the same cell, same condition is in the same row and column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get persistent cells that are initially place coding (ipc) and not initially place coding (nipc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_ipc = np.where(p_vals_persistent[:,0] <= 0.05)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_lengths_ipc = tv_lengths_persistent[i_ipc]\n",
    "tv_angles_ipc = tv_angles_persistent[i_ipc]\n",
    "p_vals_ipc = p_vals_persistent[i_ipc]\n",
    "assignments_ipc = assignments_persistent[i_ipc]\n",
    "\n",
    "tv_lengths_nipc = tv_lengths_persistent[~i_ipc]\n",
    "tv_angles_nipc = tv_angles_persistent[~i_ipc]\n",
    "p_vals_nipc = p_vals_persistent[~i_ipc]\n",
    "assignments_nipc = assignments_persistent[~i_ipc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate mean event rate (average over all cells) per condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for seaborn\n",
    "# columns: condition (bl, 30min, 60min...); cell type (npc, pc, low activity (la)); event rate\n",
    "# This includes all cells, not just omnipresent/persistent cells!\n",
    "col_event_rates = []\n",
    "col_cell_types = []\n",
    "col_conds = []\n",
    "col_cell_idxs = []\n",
    "\n",
    "for i_cond in range(len(ssm_event_masks)):\n",
    "    npc_mask = p_vals[i_cond] > 0.05\n",
    "    pc_mask = p_vals[i_cond] <= 0.05\n",
    "    q_mask = np.isnan(p_vals[i_cond])\n",
    "    # shape of ssm_event_masks[i_cond][mask]: (n_masked_cells, n_rounds, n_bins)\n",
    "    # sum up events for each round (i.e. sum up bins, axis=2), calculate average over rounds (i.e. over axis=1)\n",
    "    event_rate_npc = np.mean(np.sum(ssm_event_masks[i_cond][npc_mask], axis=2), axis=1)\n",
    "    event_rate_pc = np.mean(np.sum(ssm_event_masks[i_cond][pc_mask], axis=2), axis=1)\n",
    "    event_rate_la = np.mean(np.sum(ssm_event_masks[i_cond][q_mask], axis=2), axis=1)\n",
    "\n",
    "    # get the index of the cells in the first condition (baseline)\n",
    "    idx_npc = assignments[np.argsort(assignments[:,i_cond])][np.where(npc_mask)[0]][:,0]\n",
    "    idx_pc = assignments[np.argsort(assignments[:,i_cond])][np.where(pc_mask)[0]][:,0]\n",
    "    idx_q = assignments[np.argsort(assignments[:,i_cond])][np.where(q_mask)[0]][:,0]\n",
    "\n",
    "\n",
    "    col_event_rates.extend(event_rate_npc)\n",
    "    col_conds.extend([conditions[i_cond]]*len(event_rate_npc))\n",
    "    col_cell_types.extend([\"npc\"]*len(event_rate_npc))\n",
    "    col_cell_idxs.extend(idx_npc)\n",
    "\n",
    "    col_event_rates.extend(event_rate_pc)\n",
    "    col_conds.extend([conditions[i_cond]]*len(event_rate_pc))\n",
    "    col_cell_types.extend([\"pc\"]*len(event_rate_pc))\n",
    "    col_cell_idxs.extend(idx_pc)\n",
    "    \n",
    "    col_event_rates.extend(event_rate_la)\n",
    "    col_conds.extend([conditions[i_cond]]*len(event_rate_la))\n",
    "    col_cell_types.extend([\"la\"]*len(event_rate_la))\n",
    "    col_cell_idxs.extend(idx_q)\n",
    "\n",
    "\n",
    "df_event_rates = pd.DataFrame({\"condition\": col_conds, \"cell_type\": col_cell_types, \"event_rate\": col_event_rates, \"cell_bl_id\": col_cell_idxs})  # cell_bl_id must be float because of the np.NaNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(conditions), sharey=True, sharex=True, figsize=(24, 10))\n",
    "plt.suptitle(mouse_id)\n",
    "for i_cond in range(len(conditions)):\n",
    "    sns.histplot(\n",
    "        df_event_rates[(df_event_rates[\"condition\"] == conditions[i_cond])],\n",
    "        x=\"event_rate\", hue=\"cell_type\",\n",
    "        multiple=\"layer\",\n",
    "        edgecolor=\".3\",\n",
    "        linewidth=.5,\n",
    "        log_scale=(False, True),\n",
    "        ax=axs[i_cond]\n",
    "    )\n",
    "    axs[i_cond].set_title(f'{conditions[i_cond]}, n_q={len(df_event_rates[(df_event_rates[\"condition\"] == conditions[i_cond]) & (df_event_rates[\"cell_type\"] == \"la\")])}')\n",
    "if save_figs:\n",
    "    out_fpath = os.path.join(output_folder, f\"pca_{mouse_id}_hist_cell_types_{datetime_str}{file_extension}\")\n",
    "    plt.savefig(out_fpath)\n",
    "    print(f\"Saved to {out_fpath}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot in 3D space the event rate of each unit\n",
    "Two methods:\n",
    "1. only keep persistent units\n",
    "2. replace nan with 0 (ansatz: not identified cells were not firing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(conditions) == 3:\n",
    "    def get_event_rate(grp, cond):\n",
    "        er = grp[grp[\"condition\"] == cond][\"event_rate\"]\n",
    "        if len(er) > 0:\n",
    "            assert len(er) == 1  # assert unique event rate for condition and cell id\n",
    "            return er.iloc[0]\n",
    "        else:\n",
    "            return 0  # assume event rate is 0 if cell was not identified for specified condition\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "    zs = []\n",
    "    cids = []\n",
    "    for i, g in df_event_rates.groupby(\"cell_bl_id\"):\n",
    "        # get x, y, z coordinates as event rates for each cell in bl, 30min, 60min (or cond[0], cond[1], cond[2])\n",
    "        # if a condition is missing, fill in event rate as 0\n",
    "        x = get_event_rate(g, conditions[0])\n",
    "        y = get_event_rate(g, conditions[1])\n",
    "        z = get_event_rate(g, conditions[2])\n",
    "\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        zs.append(z)\n",
    "        cids.append(i)\n",
    "\n",
    "    df_matched_event_rates = pd.DataFrame({\"cell_bl_id\": cids, \"event_rate_0\": xs, \"event_rate_1\": ys, \"event_rate_2\": zs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(conditions) == 3:\n",
    "    df_matched_event_rates_filt = df_matched_event_rates[df_matched_event_rates[\"event_rate_0\"] != 0]  # filter non-zero baseline event rate\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    plt.scatter(df_matched_event_rates_filt[\"event_rate_1\"]/df_matched_event_rates_filt[\"event_rate_0\"], df_matched_event_rates_filt[\"event_rate_2\"]/df_matched_event_rates_filt[\"event_rate_0\"])\n",
    "    ax = plt.gca()\n",
    "    plt.suptitle(\"Event rate ratios\")\n",
    "    ax.set_xlabel(f'{conditions[1]}/{conditions[0]}')\n",
    "    ax.set_ylabel(f'{conditions[2]}/{conditions[0]}')\n",
    "    if save_figs:\n",
    "        out_fpath = os.path.join(output_folder, f\"pca_{mouse_id}_event_rate_ratios_{datetime_str}{file_extension}\")\n",
    "        plt.savefig(out_fpath)\n",
    "        print(f\"Saved to {out_fpath}\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check movement between place-coding, non-place-coding, low activity cells\n",
    "Low activity (la) cells: cells that were not included in PC analysis (minimum event number criterion not fulfilled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silent cells appear as np.nan in p_vals. Make sure they return FALSE for both PC and nPC conditions\n",
    "assert not(np.nan > 0.05)\n",
    "assert not(np.nan <= 0.05)\n",
    "assert np.isnan(np.nan)\n",
    "\n",
    "labels = [] \n",
    "colors = []\n",
    "for condition in conditions:\n",
    "  labels.extend([f\"PC {condition}\", f\"nPC {condition}\", f\"la {condition}\"])  # for each condition, check categories PC, not-PC and low activity\n",
    "  colors.extend([\"red\", \"blue\", \"grey\"])  # 255, 0, 0;  0, 255, 0; 0, 0, 0\n",
    "# in each condition, we have PC and nPC categories, each have PC and nPC targets in the next category\n",
    "sources = []  # should be 0, 1, 2, 0, 1, 2, 0, 1, 2, 3, 4, 5, 3, 4, 5, 3, 4, 5, ...\n",
    "targets = []  # should be 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, ...\n",
    "values = []\n",
    "link_colors = []\n",
    "for i_condition in range(n_conditions-1):  # last condition does not have output\n",
    "  # PC, nPC and S sources flow to PC in target\n",
    "  # i. e. PC[i_condition] -> PC[i_condition+1], nPC[i_condition] -> PC[i_condition+1], Q[i_condition] -> PC[i_condition+1]\n",
    "  n_PC_to_PC = np.sum(np.logical_and(p_vals_paired[:,i_condition] <= 0.05,  p_vals_paired[:,i_condition+1] <= 0.05))\n",
    "  n_nPC_to_PC = np.sum(np.logical_and(p_vals_paired[:,i_condition] > 0.05,  p_vals_paired[:,i_condition+1] <= 0.05))\n",
    "  n_la_to_PC = np.sum(np.logical_and(np.isnan(p_vals_paired[:, i_condition]), p_vals_paired[:,i_condition+1] <= 0.05))\n",
    "  sources.extend([3*i_condition,3*i_condition+1, 3*i_condition+2])\n",
    "  targets.extend([3*(i_condition+1), 3*(i_condition+1), 3*(i_condition+1)])\n",
    "  values.extend([n_PC_to_PC, n_nPC_to_PC, n_la_to_PC])\n",
    "  link_colors.extend([\"rgba(255, 0, 0, 0.4)\", \"rgba(0, 0, 255, 0.4)\", \"rgba(0, 0, 0, 0.4)\"])  # PC -> x is light blue, nPC -> x is light red, Q -> y is \"light black\"\n",
    "\n",
    "  # PC, nPC and Q sources flow to nPC in target\n",
    "  # i. e. PC[i_condition] -> nPC[i_condition+1], nPC[i_condition] -> nPC[i_condition+1], Q[i_condition] -> nPC[i_condition+1]\n",
    "  n_PC_to_nPC = np.sum(np.logical_and(p_vals_paired[:,i_condition] <= 0.05,  p_vals_paired[:,i_condition+1] > 0.05))\n",
    "  n_nPC_to_nPC = np.sum(np.logical_and(p_vals_paired[:,i_condition] > 0.05,  p_vals_paired[:,i_condition+1] > 0.05))\n",
    "  n_la_to_nPC = np.sum(np.logical_and(np.isnan(p_vals_paired[:, i_condition]), p_vals_paired[:,i_condition+1] > 0.05))\n",
    "  sources.extend([3*i_condition,3*i_condition+1, 3*i_condition+2])\n",
    "  targets.extend([3*(i_condition+1)+1, 3*(i_condition+1)+1, 3*(i_condition+1)+1])\n",
    "  values.extend([n_PC_to_nPC, n_nPC_to_nPC, n_la_to_nPC])\n",
    "  link_colors.extend([\"rgba(255, 0, 0, 0.4)\", \"rgba(0, 0, 255, 0.4)\", \"rgba(0, 0, 0, 0.4)\"])  # PC -> x is light blue, nPC -> x is light red, Q -> y is \"light black\"\n",
    "\n",
    "\n",
    "  # PC, nPC and Q sources flow to S in target\n",
    "  # i. e. PC[i_condition] -> Q[i_condition+1], nPC[i_condition] -> Q[i_condition+1], Q[i_condition] -> Q[i_condition+1]\n",
    "  n_PC_to_la = np.sum(np.logical_and(p_vals_paired[:,i_condition] <= 0.05,  np.isnan(p_vals_paired[:,i_condition+1]) ))\n",
    "  n_nPC_to_la = np.sum(np.logical_and(p_vals_paired[:,i_condition] > 0.05,  np.isnan(p_vals_paired[:,i_condition+1])  ))\n",
    "  n_la_to_la = np.sum(np.logical_and(np.isnan(p_vals_paired[:, i_condition]), np.isnan(p_vals_paired[:,i_condition+1])  ))\n",
    "  sources.extend([3*i_condition,3*i_condition+1, 3*i_condition+2])\n",
    "  targets.extend([3*(i_condition+1)+2, 3*(i_condition+1)+2, 3*(i_condition+1)+2])\n",
    "  values.extend([n_PC_to_la, n_nPC_to_la, n_la_to_la])\n",
    "  link_colors.extend([\"rgba(255, 0, 0, 0.4)\", \"rgba(0, 0, 255, 0.4)\", \"rgba(0, 0, 0, 0.4)\"])  # PC -> x is light blue, nPC -> x is light red, Q -> y is \"light black\"\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 20,\n",
    "      line = dict(color = \"black\", width = 0.5),\n",
    "      label = labels,\n",
    "      color = colors\n",
    "    ),\n",
    "    link = dict(\n",
    "      source = sources, # indices correspond to labels, eg A1, A2, A1, B1, ...\n",
    "      target = targets,\n",
    "      value = values,\n",
    "      color = link_colors\n",
    "  ))])\n",
    "\n",
    "fig.update_layout(title_text=\"Place coding (PC) - non-place coding (nPC) - low activity (la)\", font_size=10)\n",
    "\n",
    "if save_figs:\n",
    "    out_fpath = os.path.join(output_folder, f\"pca_{mouse_id}_sankey_{datetime_str}.html\")\n",
    "    out_fpath_original_ext = os.path.join(output_folder, f\"pca_{mouse_id}_sankey_{datetime_str}{file_extension}\")\n",
    "    fig.write_html(out_fpath)\n",
    "    fig.write_image(out_fpath_original_ext)  # requires kaleido package\n",
    "    print(f\"Saved to {out_fpath}\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## % of place cells at each time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_cell_ratio = np.zeros(len(p_vals))  # the % of place cells for each condition\n",
    "for i_condition in range(len(p_vals)):\n",
    "    place_cell_ratio[i_condition] = np.sum(p_vals[i_condition] <= 0.05)/len(p_vals[i_condition])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8))\n",
    "axs[0].plot(place_cell_ratio*100.)\n",
    "axs[0].set_xticks(range(len(conditions)), conditions)\n",
    "axs[0].set_ylabel(\"% place cells\")\n",
    "\n",
    "axs[1].plot([len(p_vals[i_cond]) for i_cond in range(len(p_vals))])\n",
    "axs[1].set_xticks(range(len(conditions)), conditions)\n",
    "axs[1].set_ylabel(\"# cells (total)\")\n",
    "if save_figs:\n",
    "    out_fpath = os.path.join(output_folder, f\"pca_{mouse_id}_percent_{datetime_str}{file_extension}\")\n",
    "    plt.savefig(out_fpath)\n",
    "    print(f\"Saved to {out_fpath}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check persistent cells behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [] \n",
    "colors= []\n",
    "for condition in conditions:\n",
    "  labels.extend([f\"PC {condition}\", f\"nPC {condition}\"])  # for each condition, check categories PC and not-PC\n",
    "  colors.extend([\"red\", \"blue\"])\n",
    "# in each condition, we have PC and nPC categories, each have PC and nPC targets in the next category\n",
    "sources = []  # should be 0, 1, 0, 1, 2, 3, 2, 3, ...\n",
    "targets = []  # should be 2, 3, 2, 3, 4, 5, 4, 5, ...\n",
    "values = []\n",
    "link_colors = []\n",
    "for i_condition in range(len(conditions)-1):  # last condition does not have output\n",
    "  # PC and nPC sources flow to PC in target\n",
    "  # i. e. PC[i_condition] -> PC[i_condition+1], nPC[i_condition] -> PC[i_condition+1]\n",
    "  n_PC_to_PC = np.sum(np.logical_and(p_vals_persistent[:,i_condition] <= 0.05,  p_vals_persistent[:,i_condition+1] <= 0.05))\n",
    "  n_nPC_to_PC = np.sum(np.logical_and(p_vals_persistent[:,i_condition] > 0.05,  p_vals_persistent[:,i_condition+1] <= 0.05))\n",
    "  sources.extend([2*i_condition,2*i_condition+1])\n",
    "  targets.extend([2*(i_condition+1), 2*(i_condition+1)])\n",
    "  values.extend([n_PC_to_PC, n_nPC_to_PC])\n",
    "  link_colors.extend([\"rgba(255, 0, 0, 0.4)\", \"rgba(0, 0, 255, 0.4)\"])  # PC -> x is light blue, nPC -> x is light red\n",
    "  \n",
    "  # PC and nPC sources flow to nPC in target\n",
    "  # i. e. PC[i_condition] -> nPC[i_condition+1], nPC[i_condition] -> nPC[i_condition+1]\n",
    "  n_PC_to_nPC = np.sum(np.logical_and(p_vals_persistent[:,i_condition] <= 0.05,  p_vals_persistent[:,i_condition+1] > 0.05))\n",
    "  n_nPC_to_nPC = np.sum(np.logical_and(p_vals_persistent[:,i_condition] > 0.05,  p_vals_persistent[:,i_condition+1] > 0.05))\n",
    "  sources.extend([2*i_condition,2*i_condition+1])\n",
    "  targets.extend([2*(i_condition+1)+1, 2*(i_condition+1)+1])\n",
    "  values.extend([n_PC_to_nPC, n_nPC_to_nPC])\n",
    "  link_colors.extend([\"rgba(255, 0, 0, 0.4)\", \"rgba(0, 0, 255, 0.4)\"])  # PC -> x is light blue, nPC -> x is light red\n",
    "\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 20,\n",
    "      line = dict(color = \"black\", width = 0.5),\n",
    "      label = labels,\n",
    "      color = colors\n",
    "    ),\n",
    "    link = dict(\n",
    "      source = sources, # indices correspond to labels, eg A1, A2, A1, B1, ...\n",
    "      target = targets,\n",
    "      value = values,\n",
    "      color=link_colors\n",
    "  ))])\n",
    "\n",
    "fig.update_layout(title_text=\"Place coding (PC) - non-place coding (nPC) of persistent cells\", font_size=10)\n",
    "fig.write_html(\"D:\\\\Downloads\\\\pc_npc.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check tuning vector direction change/stability for initially place-coding cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={'projection': 'polar'}, figsize=(12,12))\n",
    "#ax.set_yscale('log')\n",
    "for angles in tv_angles_ipc:\n",
    "    radii = [i+1 for i in range(len(angles))]#tv_vector_lengths_paired[i_unit]\n",
    "    ax.plot(angles, radii, linewidth=0.3, marker='o')  # -pi to pi\n",
    "if save_figs:\n",
    "    out_fpath = os.path.join(output_folder, f\"pca_{mouse_id}_directions_{datetime_str}{file_extension}\")\n",
    "    plt.savefig(out_fpath)\n",
    "    print(f\"Saved to {out_fpath}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ssm of initial place coding cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm_z_ipc = [ssm_zs[i_cond][assignments_ipc[:,i_cond]] for i_cond in range(n_conditions)]\n",
    "# average over rounds\n",
    "ssm_z_ipc_avg = [np.average(ssm_z_ipc[i_cond], axis=1) for i_cond in range(n_conditions)]\n",
    "# order cells by maximum of avg ssm in baseline\n",
    "# 1. find index of maximum in ssm for each cell\n",
    "# 2. sort cell indices by the corresponding entries in ascending order\n",
    "idx_onset_sorted = [np.argsort(np.argmax(ssm_z_ipc_avg[i_cond], axis=1)) for i_cond in range(n_conditions)]\n",
    "ssm_z_ipc_avg_sorted = [ssm_z_ipc_avg[i_cond][idx_onset_sorted[i_cond]] for i_cond in range(n_conditions)]\n",
    "# smooth each row of ssm\n",
    "ssm_z_ipc_avg_sorted_smooth = ssm_z_ipc_avg_sorted.copy()\n",
    "for i_row in range(len(ssm_z_ipc_avg_sorted_smooth)):\n",
    "    ssm_z_ipc_avg_sorted_smooth[i_row]= gaussian_filter1d(ssm_z_ipc_avg_sorted_smooth[i_row], sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, n_conditions, figsize=(18, 8))\n",
    "plt.suptitle(f\"{mouse_id} initial PC\")\n",
    "for i_cond in range(n_conditions):\n",
    "    axs[i_cond].imshow(ssm_z_ipc_avg_sorted_smooth[i_cond], aspect=\"auto\", cmap=\"jet\")\n",
    "    axs[i_cond].title.set_text(conditions[i_cond])\n",
    "if save_figs and False:\n",
    "    fig_fpath = os.path.join(output_folder, f\"pca_{mouse_id}_ssm_ipc_gsig={sigma}_\"+datetime_str+file_extension)\n",
    "    plt.savefig(fig_fpath)\n",
    "    print(f\"Saved to {fig_fpath}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ssm of all persistent cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm_z_persistent = [ssm_zs[i_cond][assignments_persistent[:,i_cond]] for i_cond in range(n_conditions)]\n",
    "# average over rounds\n",
    "ssm_z_persistent_avg = [np.average(ssm_z_persistent[i_cond], axis=1) for i_cond in range(n_conditions)]\n",
    "# order cells by maximum of avg ssm in baseline\n",
    "# 1. find index of maximum in ssm for each cell\n",
    "# 2. sort cell indices by the corresponding entries in ascending order\n",
    "idx_onset_sorted = [np.argsort(np.argmax(ssm_z_persistent_avg[i_cond], axis=1)) for i_cond in range(n_conditions)]\n",
    "ssm_z_persistent_avg_sorted = [ssm_z_persistent_avg[i_cond][idx_onset_sorted[i_cond]] for i_cond in range(n_conditions)]\n",
    "\n",
    "ssm_z_persistent_avg_sorted_smooth = ssm_z_persistent_avg_sorted.copy()\n",
    "for i_row in range(len(ssm_z_persistent_avg_sorted_smooth)):\n",
    "    ssm_z_persistent_avg_sorted_smooth[i_row]= gaussian_filter1d(ssm_z_persistent_avg_sorted_smooth[i_row], sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, n_conditions, figsize=(18, 8))\n",
    "plt.suptitle(f\"{mouse_id} persistent\")\n",
    "for i_cond in range(n_conditions):\n",
    "    axs[i_cond].imshow(ssm_z_persistent_avg_sorted_smooth[i_cond], aspect=\"auto\", cmap=\"jet\")\n",
    "    axs[i_cond].title.set_text(conditions[i_cond])\n",
    "if save_figs:\n",
    "    fig_fpath = os.path.join(output_folder, f\"pca_{mouse_id}_ssm_persistent_gsig={sigma}_\"+datetime_str+file_extension)\n",
    "    plt.savefig(fig_fpath)\n",
    "    print(f\"Saved to {fig_fpath}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save multisession tracking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spatial_union, assignments, matchings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
